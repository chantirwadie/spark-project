{
  "metadata": {
    "name": "training model sentiment analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "Getting data"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import monotonically_increasing_id"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport time\nfrom pyspark import SparkContext,SparkConf\nfrom pyspark.sql import SparkSession\n\nappName\u003d \"hive_pyspark\"\n\nconf\u003dSparkConf()\nsc\u003dSparkContext.getOrCreate(conf\u003dconf)\nspark \u003d SparkSession(sc).builder.appName(appName).config(\"hive.metastore.uris\",\"thrift://hive-metastore:9083\").enableHiveSupport().getOrCreate()\n\nr1\u003d[]\nr2\u003d[]\nr3\u003d[]\nr4\u003d[]\nr5\u003d[]\nr6\u003d[]\n\ndf\u003dspark.read.csv(\"data.csv\",header\u003d\u0027true\u0027)\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d df.dropna()\ndf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n(train_set, val_set, test_set) \u003d df.randomSplit([0.98, 0.01, 0.01], seed \u003d 2000)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\n\ntokenizer \u003d Tokenizer(inputCol\u003d\"Sentence\", outputCol\u003d\"words\")\nhashtf \u003d HashingTF(numFeatures\u003d2**16, inputCol\u003d\"words\", outputCol\u003d\u0027tf\u0027)\nidf \u003d IDF(inputCol\u003d\u0027tf\u0027, outputCol\u003d\"features\", minDocFreq\u003d5) #minDocFreq: remove sparse terms\nlabel_stringIdx \u003d StringIndexer(inputCol \u003d \"Sentiment\", outputCol \u003d \"label\")\npipeline \u003d Pipeline(stages\u003d[tokenizer, hashtf, idf, label_stringIdx])\n\npipelineFit \u003d pipeline.fit(train_set)\ntrain_df \u003d pipelineFit.transform(train_set)\nval_df \u003d pipelineFit.transform(val_set)\ntrain_df.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nval_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.classification import LogisticRegression\nlr \u003d LogisticRegression(maxIter\u003d100)\nlrModel \u003d lr.fit(train_df)\npredictionsLr \u003d lrModel.transform(val_df)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.classification import NaiveBayes\n\nnb \u003d NaiveBayes(smoothing\u003d1.0, modelType\u003d\"multinomial\")\nnbModel \u003d nb.fit(train_df)\npredictionsNb \u003d nbModel.transform(val_df)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\ndt \u003d DecisionTreeClassifier(maxDepth\u003d2)\n\ndtModel \u003d dt.fit(train_df)\npredictionsDt \u003d dtModel.transform(val_df)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictionsLr.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictionsNb.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictionsDt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracy \u003d predictionsLr.filter(predictionsLr.label \u003d\u003d predictionsLr.prediction).count() / float(val_set.count())\naccuracy\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nroc_auc \u003d evaluator.evaluate(predictionsLr)\nroc_auc"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracy \u003d predictionsNb.filter(predictionsNb.label \u003d\u003d predictionsNb.prediction).count() / float(val_set.count())\naccuracy"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nroc_auc \u003d evaluator.evaluate(predictionsNb)\nroc_auc"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracy \u003d predictionsDt.filter(predictionsDt.label \u003d\u003d predictionsDt.prediction).count() / float(val_set.count())\naccuracy"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nroc_auc \u003d evaluator.evaluate(predictionsDt)\nroc_auc"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n%%time\ntrigramwocs_pipelineFit \u003d build_ngrams_wocs().fit(train_set)\npredictions_wocs \u003d trigramwocs_pipelineFit.transform(val_set)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracy_wocs \u003d predictions_wocs.filter(predictions_wocs.label \u003d\u003d predictions_wocs.prediction).count() / float(val_set.count())\naccuracy_wocs"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nappName\u003d \"hive_pyspark\"\n\nconf\u003dSparkConf()\nsc\u003dSparkContext.getOrCreate(conf\u003dconf)\nspark \u003d SparkSession(sc).builder.appName(appName).config(\"hive.metastore.uris\",\"thrift://hive-metastore:9083\").enableHiveSupport().getOrCreate()\n\nr1\u003d[]\nr2\u003d[]\nr3\u003d[]\nr4\u003d[]\nr5\u003d[]\nr6\u003d[]\n\ntweet\u003dspark.read.parquet(\"tweet09-12-2022.parquet.gzip\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntweet.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntweetset \u003d pipelineFit.transform(datafile)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntweetset.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictionsTweet \u003d lrModel.transform(tweetset)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictionsTweet.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nAVGsentimentTweet \u003d predictionsTweet.select(mean(\u0027prediction\u0027)).collect()[0][0]\nAVGsentimentTweet"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictionsTweet.groupBy(\"prediction\").count().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnumPositveTweet\u003dpredictionsTweet.groupBy(\"prediction\").count().collect()[2][1]\nnumPositveTweet"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sql(\"show tables\").show"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sql(\"select * from my_temp_table\").show                   "
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nappName\u003d \"hive_pyspark\"\n\nconf\u003dSparkConf()\nsc\u003dSparkContext.getOrCreate(conf\u003dconf)\nspark \u003d SparkSession(sc).builder.appName(appName).config(\"hive.metastore.uris\",\"thrift://hive-metastore:9083\").enableHiveSupport().getOrCreate()\n\nr1\u003d[]\nr2\u003d[]\nr3\u003d[]\nr4\u003d[]\nr5\u003d[]\nr6\u003d[]\n\narticles\u003dspark.read.parquet(\"articles-2022-12-13.parquet.gzip\")"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\narticles.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nArticleSet \u003d pipelineFit.transform(articles)"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nArticlesPredictions \u003d nbModel.transform(ArticleSet)"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nArticlesPredictions.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nAVGsentimentNew \u003d ArticlesPredictions.select(mean(\u0027prediction\u0027)).collect()[0][0]\nAVGsentimentNew"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nArticlesPredictions.groupBy(\"prediction\").count().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnumPositveNew\u003dArticlesPredictions.groupBy(\"prediction\").count().collect()[1][1]\nnumPositveNew"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nappName\u003d \"hive_pyspark\"\n\nconf\u003dSparkConf()\nsc\u003dSparkContext.getOrCreate(conf\u003dconf)\nspark \u003d SparkSession(sc).builder.appName(appName).config(\"hive.metastore.uris\",\"thrift://hive-metastore:9083\").enableHiveSupport().getOrCreate()\n\nr1\u003d[]\nr2\u003d[]\nr3\u003d[]\nr4\u003d[]\nr5\u003d[]\nr6\u003d[]\n\nyahoo\u003dspark.read.parquet(\"yahoo10-12-2022.parquet.gzip\")"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nyahoo.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nb\u003d yahoo.collect()[0][1]"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d spark.createDataFrame(\n    [\n        (\"2022-12-09 16:00:00\", 179.05,179.05,179.05,179.05,179.05,1.26,0.66,1,218.0),\n        (\"2022-12-08 12:00:00\", 162.33,162.33,162.33,162.33,162.33,1.46,0.75,3,240.0),\n        (\"2022-12-07 11:00:00\", 169.15,169.15,169.15,169.15,169.15,1.12,0.5,2,191.0),\n        (\"2022-12-06 10:00:00\", 139.05,139.05,139.05,139.05,139.05,1.36,0.2,3,142.0),\n        (\"2022-12-05 22:00:00\", 174.44,174.44,174.44,174.44,174.44,1.26,0.,1,148.0),\n        (\"2022-12-04 17:00:00\", 166.77,166.77,166.77,166.77,166.77,1.11,0.25,2,238.0),\n        (\"2022-12-03 16:00:00\", 169.02,169.02,169.02,169.02,169.02,1.22,0.5,2,188.0),\n        (\"2022-12-02 21:00:00\", 155.12,155.12,155.12,155.12,155.12,1.52,0.66,3,198.0),\n        (\"2022-12-01 11:00:00\", 139.15,139.15,139.15,139.15,139.15,1.06,0.1,4,228.0),\n        (\"2022-11-30 13:00:00\", 149.05,149.05,149.05,149.05,149.05,1.2,0.33,3,258.0),# create your data here, be consistent in the types.\n    ],\n    [\"Datetime\",\"Open\",\"High\",\"Low\",\"Close\",\"AdjClose\",\"AVGsentimentNew\",\"AVGsentimentTweet\",\"numPositveNew\",\"numPositveTweet\"]  # add your column names here\n)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}