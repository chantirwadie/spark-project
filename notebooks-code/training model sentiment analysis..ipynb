{
  "metadata": {
    "name": "training model sentiment analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "Getting data"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport time\nfrom pyspark import SparkContext,SparkConf\nfrom pyspark.sql import SparkSession\n\nappName\u003d \"hive_pyspark\"\n\nconf\u003dSparkConf()\nsc\u003dSparkContext.getOrCreate(conf\u003dconf)\nspark \u003d SparkSession(sc).builder.appName(appName).config(\"hive.metastore.uris\",\"thrift://hive-metastore:9083\").enableHiveSupport().getOrCreate()\n\nr1\u003d[]\nr2\u003d[]\nr3\u003d[]\nr4\u003d[]\nr5\u003d[]\nr6\u003d[]\n\ndf\u003dspark.read.csv(\"data.csv\",header\u003d\u0027true\u0027)\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf \u003d df.dropna()\ndf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n(train_set, val_set, test_set) \u003d df.randomSplit([0.98, 0.01, 0.01], seed \u003d 2000)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\n\ntokenizer \u003d Tokenizer(inputCol\u003d\"Sentence\", outputCol\u003d\"words\")\nhashtf \u003d HashingTF(numFeatures\u003d2**16, inputCol\u003d\"words\", outputCol\u003d\u0027tf\u0027)\nidf \u003d IDF(inputCol\u003d\u0027tf\u0027, outputCol\u003d\"features\", minDocFreq\u003d5) #minDocFreq: remove sparse terms\nlabel_stringIdx \u003d StringIndexer(inputCol \u003d \"Sentiment\", outputCol \u003d \"label\")\npipeline \u003d Pipeline(stages\u003d[tokenizer, hashtf, idf, label_stringIdx])\n\npipelineFit \u003d pipeline.fit(train_set)\ntrain_df \u003d pipelineFit.transform(train_set)\nval_df \u003d pipelineFit.transform(val_set)\ntrain_df.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nval_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.classification import LogisticRegression\nlr \u003d LogisticRegression(maxIter\u003d100)\nlrModel \u003d lr.fit(train_df)\npredictions \u003d lrModel.transform(val_df)\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator \u003d BinaryClassificationEvaluator(rawPredictionCol\u003d\"rawPrediction\")\nevaluator.evaluate(predictions)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracy \u003d predictions.filter(predictions.label \u003d\u003d predictions.prediction).count() / float(val_set.count())\naccuracy"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n%%time\nfrom pyspark.ml.feature import CountVectorizer\n\ntokenizer \u003d Tokenizer(inputCol\u003d\"Sentence\", outputCol\u003d\"words\")\ncv \u003d CountVectorizer(vocabSize\u003d2**16, inputCol\u003d\"words\", outputCol\u003d\u0027cv\u0027)\nidf \u003d IDF(inputCol\u003d\u0027cv\u0027, outputCol\u003d\"features\", minDocFreq\u003d5) #minDocFreq: remove sparse terms\nlabel_stringIdx \u003d StringIndexer(inputCol \u003d \"Sentiment\", outputCol \u003d \"label\")\nlr \u003d LogisticRegression(maxIter\u003d100)\npipeline \u003d Pipeline(stages\u003d[tokenizer, cv, idf, label_stringIdx, lr])\n\npipelineFit \u003d pipeline.fit(train_set)\npredictions \u003d pipelineFit.transform(val_set)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictions.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracy \u003d predictions.filter(predictions.label \u003d\u003d predictions.prediction).count() / float(val_set.count())\naccuracy\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nroc_auc \u003d evaluator.evaluate(predictions)\nroc_auc"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import NGram, VectorAssembler\nfrom pyspark.ml.feature import ChiSqSelector\n\ndef build_trigrams(inputCol\u003d[\"Sentence\",\"Sentiment\"], n\u003d3):\n    tokenizer \u003d [Tokenizer(inputCol\u003d\"Sentence\", outputCol\u003d\"words\")]\n    ngrams \u003d [\n        NGram(n\u003di, inputCol\u003d\"words\", outputCol\u003d\"{0}_grams\".format(i))\n        for i in range(1, n + 1)\n    ]\n\n    cv \u003d [\n        CountVectorizer(vocabSize\u003d2**14,inputCol\u003d\"{0}_grams\".format(i),\n            outputCol\u003d\"{0}_tf\".format(i))\n        for i in range(1, n + 1)\n    ]\n    idf \u003d [IDF(inputCol\u003d\"{0}_tf\".format(i), outputCol\u003d\"{0}_tfidf\".format(i), minDocFreq\u003d5) for i in range(1, n + 1)]\n\n    assembler \u003d [VectorAssembler(\n        inputCols\u003d[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n        outputCol\u003d\"rawFeatures\"\n    )]\n    label_stringIdx \u003d [StringIndexer(inputCol \u003d \"Sentiment\", outputCol \u003d \"label\")]\n    selector \u003d [ChiSqSelector(numTopFeatures\u003d2**14,featuresCol\u003d\u0027rawFeatures\u0027, outputCol\u003d\"features\")]\n    lr \u003d [LogisticRegression(maxIter\u003d100)]\n    return Pipeline(stages\u003dtokenizer + ngrams + cv + idf+ assembler + label_stringIdx+selector+lr)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef build_ngrams_wocs(inputCol\u003d[\"text\",\"target\"], n\u003d3):\n    tokenizer \u003d [Tokenizer(inputCol\u003d\"Sentence\", outputCol\u003d\"words\")]\n    ngrams \u003d [\n        NGram(n\u003di, inputCol\u003d\"words\", outputCol\u003d\"{0}_grams\".format(i))\n        for i in range(1, n + 1)\n    ]\n\n    cv \u003d [\n        CountVectorizer(vocabSize\u003d5460,inputCol\u003d\"{0}_grams\".format(i),\n            outputCol\u003d\"{0}_tf\".format(i))\n        for i in range(1, n + 1)\n    ]\n    idf \u003d [IDF(inputCol\u003d\"{0}_tf\".format(i), outputCol\u003d\"{0}_tfidf\".format(i), minDocFreq\u003d5) for i in range(1, n + 1)]\n\n    assembler \u003d [VectorAssembler(\n        inputCols\u003d[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n        outputCol\u003d\"features\"\n    )]\n    label_stringIdx \u003d [StringIndexer(inputCol \u003d \"Sentiment\", outputCol \u003d \"label\")]\n    lr \u003d [LogisticRegression(maxIter\u003d100)]\n    return Pipeline(stages\u003dtokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n%%time\ntrigramwocs_pipelineFit \u003d build_ngrams_wocs().fit(train_set)\npredictions_wocs \u003d trigramwocs_pipelineFit.transform(val_set)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracy_wocs \u003d predictions_wocs.filter(predictions_wocs.label \u003d\u003d predictions_wocs.prediction).count() / float(val_set.count())\naccuracy_wocs"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nroc_auc_wocs \u003d evaluator.evaluate(predictions_wocs)# print accuracy, roc_auc\nroc_auc_wocs"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nappName\u003d \"hive_pyspark\"\n\nconf\u003dSparkConf()\nsc\u003dSparkContext.getOrCreate(conf\u003dconf)\nspark \u003d SparkSession(sc).builder.appName(appName).config(\"hive.metastore.uris\",\"thrift://hive-metastore:9083\").enableHiveSupport().getOrCreate()\n\nr1\u003d[]\nr2\u003d[]\nr3\u003d[]\nr4\u003d[]\nr5\u003d[]\nr6\u003d[]\n\ndatafile\u003dspark.read.parquet(\"tweet09-12-2022.parquet.gzip\")"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndatafile.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictions2 \u003d pipelineFit.transform(datafile)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictions2.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sql(\"show tables\").show"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sql(\"select * from my_temp_table\").show                   "
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nappName\u003d \"hive_pyspark\"\n\nconf\u003dSparkConf()\nsc\u003dSparkContext.getOrCreate(conf\u003dconf)\nspark \u003d SparkSession(sc).builder.appName(appName).config(\"hive.metastore.uris\",\"thrift://hive-metastore:9083\").enableHiveSupport().getOrCreate()\n\nr1\u003d[]\nr2\u003d[]\nr3\u003d[]\nr4\u003d[]\nr5\u003d[]\nr6\u003d[]\n\narticles\u003dspark.read.parquet(\"articles-2022-12-09.parquet.gzip\")"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\narticles.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nArticlesPredictions \u003d pipelineFit.transform(articles)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nArticlesPredictions.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nArticlesPredictions.select(\"rawPrediction\").show(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sql(\"create table my_table as select * from my_temp_table\");"
    }
  ]
}