{
  "metadata": {
    "name": "kafka",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import datetime\nimport tweepy\nfrom textblob import TextBlob\nfrom kafka import KafkaProducer\nfrom json import dumps\n \nimport pandas as pd\nfrom datetime import date\n\n \nCONSUMER_KEY \u003d \"knEDQJk2AIwVkeAXRGDocGhon\"\nCONSUMER_SECRET \u003d \"tnoCESMJvNXiEmvvSgajugw7fxGilo9YiQ8agbxJLOd727uJXi\"\nACCESS_TOKEN \u003d \"1547498191779774464-lqh6oSk1r99oWurZdmP8P2kPAtZuIV\"\nACCESS_TOKEN_SECRET \u003d \"oru0PpI4NCysH7mRBTdX4KIHgobqd8CEMjxnqE9ET3Fx3\"\ndef json_serial(obj):\n    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n\n    if isinstance(obj, (datetime, datetime.date)):\n        return obj.isoformat()\n    raise TypeError(\"Type %s not serializable\" % type(obj))\nauth \u003d tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\nauth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\nproducer \u003d KafkaProducer(bootstrap_servers\u003d[\u0027192.168.1.191:9092\u0027],value_serializer\u003dlambda K:dumps(K,default\u003djson_serial).encode(\u0027utf-8\u0027))\ncolumns \u003d [\u0027PublishedDate\u0027, \u0027User\u0027,\u0027Sentence\u0027]\ndata \u003d []\napi \u003d tweepy.API(auth)\ncursor \u003d tweepy.Cursor(api.search_tweets,q\u003d\"Tesla\",tweet_mode\u003d\u0027extended\u0027).items(50)\nfor tweet in cursor:\n   # producer.send(\u0027testTopic\u0027,tweet.full_text)\n   print(tweet.user.screen_name)\n   producer.send(\u0027testTopic\u0027,[tweet.full_text,tweet.user.screen_name])\n   data.append([tweet.created_at, tweet.user.screen_name,tweet.full_text])\ndf \u003d pd.DataFrame(data, columns\u003dcolumns)\nprint(df)\ntoday \u003d date.today()\nd1 \u003d today.strftime(\"%d-%m-%Y\")\nfilename \u003d \"tweet2\"+d1+\u0027.parquet.gzip\u0027\ndf.to_parquet(filename)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import pandas as pd\nfrom kafka import KafkaProducer\nfrom json import dumps\nfrom pynytimes import NYTAPI\nfrom datetime import date, datetime, timedelta\n\n\ndef json_serial(obj):\n    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n\n    if isinstance(obj, (datetime, date)):\n        return obj.isoformat()\n    raise TypeError(\"Type %s not serializable\" % type(obj))\n\n\n# Constants\n\nCONSUMER_KEY \u003d \"Z7s48ugCojGhc1ARbQuYZl1Rz9Zukuuc\"\nCONSUMER_SECRET \u003d \"tnoCESMJvNXiEmvvSgajugw7fxGilo9YiQ8agbxJLOd727uJXi\"\nSEARCH_TERM \u003d \"Tesla\"\nTOPIC \u003d \"news\"\n\n# Creating the producer\n\n\nproducer \u003d KafkaProducer(bootstrap_servers\u003d[\n    \u0027192.168.1.191:9092\u0027], value_serializer\u003dlambda K: dumps(K).encode(\u0027utf-8\u0027))\n\nwith NYTAPI(CONSUMER_KEY, parse_dates\u003dTrue) as nyt:\n\n    articles \u003d nyt.article_search(\n        query\u003dSEARCH_TERM,\n        results\u003d100,\n        dates\u003d{\n            \"begin\": datetime.now() - timedelta(days\u003d1),\n            \"end\": datetime.now()\n        }\n    )\n    print(len(articles))\n    serializable_viewed \u003d dumps(articles, default\u003djson_serial)\n\n    columns \u003d [\u0027Headline\u0027, \u0027Sentence\u0027,\n               \u0027PublishedDate\u0027]\n    data \u003d []\n\n    producer.send(topic\u003dTOPIC, value\u003dserializable_viewed)\n\n    for index, item in enumerate(articles):\n        # producer.send(\u0027testTopic\u0027,tweet.full_text)\n        \n        data.append([item[\u0027headline\u0027][\"main\"], item[\u0027snippet\u0027],\n                         item[\u0027pub_date\u0027]])\n\n        #if (index \u003d\u003d 1):\n            # print(dumps(item, default\u003djson_serial, sort_keys\u003dTrue, indent\u003d4))\n            #data.append([item[\u0027headline\u0027][\"main\"], item[\u0027snippet\u0027],\n                         #item[\u0027pub_date\u0027]])\n    # print(articles)\n\n        # data.append([tweet.created_at, tweet.user.screen_name])\n\n    df \u003d pd.DataFrame(data, columns\u003dcolumns)\n\n    print(datetime.now().strftime(\"%Y-%m-%d\"))\n\n    df.to_parquet(\u0027articles-\u0027+datetime.now().strftime(\"%Y-%m-%d\") +\n                  \u0027.parquet.gzip\u0027, compression\u003d\u0027gzip\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nimport time\nfrom kafka import KafkaProducer\nimport yfinance as yf\nfrom datetime import date\nimport json\n\ncurrent_date \u003d date.today()\ncompany \u003d \u0027Tesla\u0027\nproducer \u003d KafkaProducer(bootstrap_servers\u003d[\u0027192.168.1.191:9092\u0027])\ntopic_name \u003d \u0027testTopic2\u0027\n\n\ndata \u003d yf.download(\"TSLA\", period\u003d\u00271d\u0027,interval\u003d\u00272m\u0027)\n  #data \u003d yf.download(tickers\u003dcompany ,start\u003dcurrent_date,interval\u003d\u00272m\u0027)\ndata \u003d data.reset_index(drop\u003dFalse)\ndata[\u0027Datetime\u0027] \u003d data[\u0027Datetime\u0027].dt.strftime(\u0027%Y-%m-%d %H:%M:%S\u0027)\nmy_dict \u003d data.iloc[-1].to_dict()\nmsg \u003d json.dumps(my_dict)\nproducer.send(topic_name, key\u003db\u0027Tesla Stock Update\u0027, value\u003dmsg.encode())\nprint(f\"Producing to {topic_name}\")\nprint(f\"msg {msg.encode()}\")\nproducer.flush()\n"
    }
  ]
}